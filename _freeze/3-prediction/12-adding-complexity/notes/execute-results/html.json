{
  "hash": "cb968d6bfd0b9c0c61f7e2ab54d8e47c",
  "result": {
    "markdown": "---\ntitle: \"Adding complexity\"\nsubtitle: \"Multiple linear regression, non-linear transformations, and polynomials.\"\ndate: \"09/30/2022\"\nimage: assets/plot3D.png\nformat:\n  html:\n    code-fold: true\n    code-link: true\n    code-summary: \".\"\nexecute: \n  warning: false\n  message: false\n  fig-width: 5\n  fig-height: 3\n  fig-align: \"center\"\n---\n\n\n[[Discuss]{.btn .btn-primary}](https://edstem.org) [[Reading Questions]{.btn .btn-primary}](https://www.gradescope.com/courses/416233)\n[[PDF]{.btn .btn-primary}](notes.pdf)\n\n\n[L]{.dropcap}ast lecture we learned how to evaluate the explanatory power of a linear model using a statistic called $r^2$. In this lecture we go on a single-minded pursuit to maximize $r^2$ by finding ways to create more accurate prediction functions. \n\n## Linear regression with multiple predictors\n\n![](images/zagat.png){fig-align=center width=\"400\"}\n\nWe often have multiple variables available from which to make predictions.\nTake restaurant pricing as an example.\n\nZagat[^zagat] is a company started in the 70s that rates restaurants  -- kind of like Yelp but the ratings are standardized and made by professionals. Here we are going to work with a dataset of Zagat ratings for 168 Italian restaurants from around Manhattan, New York. `Price` is the typical price of a meal. `Food`, `Decor`, and `Service` are all ratings for the restaurant (out of 30) and `Geo` indicates if the restaurant is on the East or West side of Manhattan.\n\n[^zagat]: [https://stories.zagat.com/](https://stories.zagat.com/)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nratings <- read_csv(\"https://www.dropbox.com/s/ebt0ypqd0m8f7vs/ratings.csv?dl=1\")\nratings\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 168 × 7\n    Case Restaurant          Price  Food Decor Service Geo  \n   <dbl> <chr>               <dbl> <dbl> <dbl>   <dbl> <chr>\n 1     1 Daniella Ristorante    43    22    18      20 West \n 2     2 Tello's Ristorante     32    20    19      19 West \n 3     3 Biricchino             34    21    13      18 West \n 4     4 Bottino                41    20    20      17 West \n 5     5 Da Umberto             54    24    19      21 West \n 6     6 Le Madri               52    22    22      21 West \n 7     7 Le Zie                 34    22    16      21 West \n 8     8 Pasticcio              34    20    18      21 East \n 9     9 Belluno                39    22    19      22 East \n10    10 Cinque Terre           44    21    17      19 East \n# … with 158 more rows\n```\n:::\n:::\n\n\nSuppose our task is to build a model to predict the price rating. Maybe we want a model that will tell us how much we will have to spend at a new restaurant that is not upfront about its pricing; or maybe we just opened a new restaurant and want to know how much customers expect to spend.\n\nA natural first attempt is to build a model to predict price from the quality of the food.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_price_form_food <- lm(Price ~ Food, data = ratings)\n\nratings_with_1d_pred <- ratings %>% \n    mutate(food_pred = predict(lm_price_form_food, ratings)) \n\nratings_with_1d_pred %>% \n    ggplot(aes(x = Food,\n               y = Price)) + \n    geom_jitter() +\n    geom_line(aes(x = Food, \n                  y = food_pred), \n              color = 'blue') +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_price_form_food\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Food, data = ratings)\n\nCoefficients:\n(Intercept)         Food  \n    -17.832        2.939  \n```\n:::\n:::\n\n\nWe can evaluate how well our linear model explains the variability in price by checking the $r^2$ value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nglance(lm_price_form_food) %>% \n    select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1     0.393\n```\n:::\n:::\n\n\n\n### Fitting lm()\n\nThe $r^2$ value tells us that we can explain 39% of the variability in the price of a meal with a linear model that uses the food quality rating of the restaurant. That's a start, but can we do better?\n\nWell we have more variables available than just the Food rating, so let's use them.\nIn other words, let's build a linear model to predict Price from the Food, Decor, Service, and Geo variables.\n\nFitting a linear model with multiple predictors is straightforward with the `lm()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlm_all <- lm(Price ~ Food + Decor + Service + Geo, data=ratings)\nlm_all\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Food + Decor + Service + Geo, data = ratings)\n\nCoefficients:\n(Intercept)         Food        Decor      Service      GeoWest  \n -21.955750     1.538120     1.910087    -0.002727    -2.068050  \n```\n:::\n:::\n\n\nThis linear model now has 5 coefficients: the intercept corresponding to restaurants on the East side, one slope parameter for each of Food, Decor, and Service, and the coefficient GeoWest that gets added to create the intercept for restaurants on the West side of 5th avenue.\n\nAs before, we evaluate our model with $R^2$. Adding additional variables to the model will always increase the $R^2$ since we are using more information.\n<!-- Adding additional variables to the model should[^overfit] make it more accurate since we are using more information. -->\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nglance(lm_all) %>% select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1     0.628\n```\n:::\n:::\n\n\n<!-- [^overfit]: The word *should* is key here; we will soon learn about (seemingly) counter-intuitive behavior where adding more information to the model actually can hurt its predictive power! -->\n<!-- This actually should not be too surprising.  -->\n<!-- Most people would do worse on the reading quiz if we give 1,000 pages of detailed notes! -->\n\nVoila! The $R^2$ for the model with 4 predictors (0.628) is better than the model with 1 predictor (0.393).\n\n### Formula\n\nAs we've seen previously, the prediction formula for a linear model with a single predictor has two coefficients.\n\n$$\n\\widehat{y} = b_0 + b_1 \\cdot x\n$$\n\nMore generally, when we have $d$ predictors $x_1, \\dots, x_d$, the linear model formula will have $d+1$ coefficients:\n\n$$\n\\widehat{y} = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 +  \\dots + b_d \\cdot x_d % = b_0 + \\sum_{j=1}^d b_j \\cdot x_j\n$$\n\nThis general form for a linear model involving multiple predictions is called *multiple linear regression*.\n\n### Geometry\n\nThe geometry of a simple linear model with one predictor was a line (blue).\nThis line is called the “prediction function” because this is what we use to predict y. Now suppose we used two predictors (e.g. Food + Decor). In this case we can still visualize the geometry. The prediction function is a two dimensional plane (blue) living in 3 dimensions; residuals (red) are measured as the vertical distance from each point to the plane.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# borrowing code from:\n# http://www.sthda.com/english/wiki/impressive-package-for-3d-and-4d-graph-r-software-and-data-visualization\n# https://github.com/idc9/stor390/blob/master/notes/linear_regression/linear_regression.Rmd\nlibrary(plot3D)\nx <- ratings$Decor\ny <- ratings$Food\nz <- ratings$Price\n\nfit <- lm(z ~ x + y)\n# predict values on regular xy grid\ngrid.lines = 26\nx.pred <- seq(min(x), max(x), length.out = grid.lines)\ny.pred <- seq(min(y), max(y), length.out = grid.lines)\nxy <- expand.grid( x = x.pred, y = y.pred)\nz.pred <- matrix(predict(fit, newdata = xy), \n                 nrow = grid.lines, ncol = grid.lines)\nfitpoints <- predict(fit)\n\n# scatter plot with regression plane\nscatter3D(x, y, z, pch = 19, cex = .5, alpha=.4, col='red', \n          theta = 200, phi =25, ticktype = \"detailed\",\n          xlab = \"Decor\", ylab = \"Food\", zlab = \"Price\",  \n          surf = list(x = x.pred, y = y.pred, z = z.pred, facets = NA, alpha=1, col='blue', fit=fitpoints), \n          main = \"\"\n          )\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=864}\n:::\n:::\n\n\nIf we use 3 or more predictors, this geometry becomes *much* harder to visualize[^hyperplane], but our formula is still perfectly intact and effective for making predictions.\n\n[^hyperplane]: If we fit a linear model with $d$ predictors, the prediction function will be a $d$ dimensional [hyperplane](https://en.wikipedia.org/wiki/Hyperplane) living in $d+1$ dimensions -- but we won't have to worry about high-dimensional geometry in this class!\n\n## Non-linear transformation\n\nThe world is not always linear. We can create *non-linear* prediction models by building off the above above *linear model* machinery. \n\n### A single non-linear term\n\nTake a question from flights lab as an example where we plot the average airspeed vs. flight distance. First lets try fitting a linear model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(stat20data)\ndata(flights)\n\nflights <- flights %>% \n    mutate(avg_speed = 60 * distance / air_time) %>%\n    drop_na(distance, air_time)\n\nlm_speed_from_dist <- lm(avg_speed ~ distance, data=flights)\n\nrsq <- glance(lm_speed_from_dist)$r.squared\n\nflights %>%\n    mutate(speed_pred = predict(lm_speed_from_dist, data=flights)) %>% \n    ggplot(aes(x=distance, y=avg_speed)) +\n    geom_point(alpha=.4) + \n    geom_line(aes(x=distance, y=speed_pred), color='blue') + \n    ggtitle(paste0(\"R squared = \", round(rsq, 3))) +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nA linear model does not seem appropriate to model average speed from distance.\nThere does appear to be a *monotonically increasing* trend, but it starts out steeper then flattens out[^concave].\nThis trend is reminiscent of functions like *log* or *square root*. \n\nLets try transforming our predictor (distance) with the log function to create a new variable called `log_dist`.\nWe can then fit a linear model using this new `log_dist` variable as the predictor.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nflights <- flights %>% \n    mutate(log_dist = log(distance))\nlm_speed_from_log_dist <- lm(avg_speed ~ log_dist, data=flights)\n```\n:::\n\n\nLooking at the data below, we see there does seem to be a linear relationship between `avg_speed` and our new variable `log_dist`!\nNotice the x-axis in the below plot is `log_dist` whereas it was `distance` in the above plot.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrsq <- glance(lm_speed_from_log_dist)$r.squared\n\nflights %>%\n    mutate(speed_pred = predict(lm_speed_from_log_dist, data=flights)) %>% \n    ggplot(aes(x=log_dist, y=avg_speed)) +\n    geom_point(alpha = .4) + \n    geom_line(aes(x=log_dist, y=speed_pred), color='blue') + \n    ggtitle(paste0(\"R squared = \", round(rsq, 3))) +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=480}\n:::\n:::\n\nThe linear model with `log_dist` ($R^2=0.843$) predicts `avg_speed` better than the linear model with `distance` ($R^2=0.72$)\n\nWe can now think of our predictive model as\n$$\n\\widehat{y} = b_0 + b_1 \\cdot \\log(x)\n$$\nIn other words, our model is *non-linear* since $x$ appears inside of a logarithm.\nWe can plot the non-linear prediction function in the original predictor distance and we see the prediction function is curved!\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nflights %>%\n    mutate(speed_pred = predict(lm_speed_from_log_dist, data=flights)) %>% \n    ggplot(aes(x=distance, y=avg_speed)) +\n    geom_point() + \n    geom_line(aes(x=distance, y=speed_pred), color='blue') +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nSo is this a linear model or a non-linear model?\nIt's both.\nWe created a new variable `log_dist` by transforming the original variable; the prediction function is a **linear** function of this new variable.\nBut we can also think of this as a function of the original variable `distance`; the prediction function is a **non-linear** function of this original variable.\n\n[^concave]: We call this concave or sometimes *diminishing marginal returns*.\n\n\n### Polynomials\n\nSometimes we need an more complex transformation than just a simple function (e.g. $\\sqrt{x}, \\log(x),  x^2,...$).\nTake the following example where there is a strong association between x and y, but it's not linear.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fix random state so the random numbers we generate are always the same\nset.seed(14)\n\nn_samples = 100\n\n# 2 * (x-1)*(x-2)*(x-5) = -20 + 34 x - 16 x^2 + 2 x^3\ndf <- tibble(x=runif(n=n_samples, min=-.5, max=5.5)) %>% \n    mutate(y=-20 + 34 * x - 16 * x^2 + 2 * x^3) %>% \n    mutate(y=y + rnorm(n=n_samples, mean = 0, sd=1.5))\n\ndf %>% \n    ggplot(aes(x=x, y=y)) + \n    geom_point() +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\nSo how should we model this?\nPolynomials to the rescue!\n\nA polynomial is a function like\n$$\nf(x) = -20 + 34 x - 16 x^2 + 2 x^3\n$$\nMore generally a polynomial is a function like\n$$\nf(x) = c_0 + c_1 \\cdot x + c_2 \\cdot x^2 + \\dots + c_d \\cdot x^d\n$$\nwhere the $d+1$ coefficients $c_0, c_1, \\dots, c_d$ are constants\nThe number $d$ is called the *degree* of the polynomial -- this is the largest exponent that appears.\n\nPolynomials are flexible functions that can be quite useful for modeling.\nWe can fit a polynomial model by adding new transformed variables to the data frame then fitting a linear model with these new transformed variables.\nThis is just like how we fit a logarithmic function before by adding a new log transformed variable to the data frame then fit a linear model.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 2\n       x      y\n   <dbl>  <dbl>\n 1  1.02 -2.61 \n 2  3.33 -7.59 \n 3  5.24  5.20 \n 4  2.82 -8.64 \n 5  5.40 11.9  \n 6  2.57 -3.53 \n 7  5.10  1.24 \n 8  2.07 -0.920\n 9  2.41 -1.36 \n10  1.79  0.853\n# … with 90 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndf_with_poly <- df %>% \n    mutate(x_sq = x^2,\n           x_cube = x^3)\n\nlm_poly <- lm(y ~ x + x_sq + x_cube, data = df_with_poly)\nlm_poly\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + x_sq + x_cube, data = df_with_poly)\n\nCoefficients:\n(Intercept)            x         x_sq       x_cube  \n    -20.086       34.669      -16.352        2.042  \n```\n:::\n:::\n\n\nNow we can plot the predictions as a function of the original x variable.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_with_poly %>% \n    mutate(y_pred = predict(lm_poly, data=df_with_poly)) %>% \n    ggplot(aes(x=x, y=y)) +\n    geom_point() + \n    geom_line(aes(x=x, y=y_pred), color='red') +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=480}\n:::\n:::\n\nThe prediction function here is a polynomial given by\n$$\n\\widehat{y} = -20.086 + 34.669 \\cdot x -16.352 \\cdot x^2 + 2.042 \\cdot x^3 \n$$\n\n### Shortcut to fitting polynomials\n\nWhat if we wanted to fit a degree 100 polynomial? \nWe could modify the code above with a really long mutate/lm function, but that is tedious. \nFortunately `R` has a nice shortcut with the `poly` function. This gives us the same 3rd degree polynomial model as above, but with much less code.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlm_poly_shortcut <- lm(y~ poly(x=x, degree=3, raw=TRUE), data=df)\nlm_poly_shortcut\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ poly(x = x, degree = 3, raw = TRUE), data = df)\n\nCoefficients:\n                         (Intercept)  poly(x = x, degree = 3, raw = TRUE)1  \n                             -20.086                                34.669  \npoly(x = x, degree = 3, raw = TRUE)2  poly(x = x, degree = 3, raw = TRUE)3  \n                             -16.352                                 2.042  \n```\n:::\n:::\n\n\n\n<!-- RAAWRRRR I cannot get interpolation to work  -->\n<!-- Ok now what does a 50 degree polynomial fit look like? -->\n<!-- ```{r} -->\n<!-- lm_poly_big <-  lm(y~ poly(x, 101, raw=T), data=df) -->\n\n<!-- df %>% -->\n<!--      mutate(y_pred = predict(lm_poly_big, data=df)) %>% -->\n<!--      ggplot(aes(x=x, y=y)) + -->\n<!--      geom_point() + -->\n<!--      geom_line(aes(x=x, y=y_pred), color='red') -->\n<!-- ``` -->\n\n\n## Summary\n\nIn this lecture we learned a couple of strategies to build more accurate models by adding complexity.\nThe first step is multiple linear regression -- fitting a linear model with multiple predictor variables.\nThis is easy to do in R with the `lm()` function and the formula is a generalization of the $\\widehat{y} = m * x + b$ formula where each predictor gets its own slope term.\nA second step is to build non-linear prediction functions.\nThere are only two ingredients for non-linear prediction functions in this lecture: 1) creating new variables with a non-linear transformation and 2) fitting a linear model with the new variables as predictors.\n\n\n<!-- {{< include ../../assets/_links-to-materials.qmd >}} -->\n\n",
    "supporting": [
      "notes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}